# 自然言語処理（NLP）の代表的なモデルとアルゴリズムの歴史

## RNNLM (2010)
- **提案者**: Tomáš Mikolov ほか
- **内容**: 自然言語処理で初めてリカレントニューラルネットワーク（RNN）を使用した言語モデル。RNNは、シーケンスデータ（例えば、文の単語列）を扱うのに適したモデルで、過去の情報を考慮しながら次の単語を予測する。
  - **RNN（リカレントニューラルネットワーク）**: 入力を順に処理し、過去の情報を内部状態に保持することで、時系列データやシーケンスデータに適用できるモデル。

## Word2Vec (2013)
- **提案者**: Tomas Mikolov ほか
- **内容**: 単語の分散表現を取得する手法。CBOWとSkip-gramの2つのアプローチを使用し、単語間の意味的な類似性を学習する。
  - **CBOW（Continuous Bag of Words）**: 周辺の単語から中央の単語を予測する手法。
  - **Skip-gram**: 中央の単語から周辺の単語を予測する手法。単語とその周辺にある単語の関係を学習するため、文脈に基づいた意味を捉えられる。

## Doc2Vec (2014)
- **提案者**: Quoc V. Le, Tomas Mikolov
- **内容**: Word2Vecの拡張版で、文書全体の特徴を捉えたベクトルを取得可能。PV-DMとPV-DBOWという2つの手法で文書ベクトルを生成する。
  - **PV-DM（Distributed Memory version of Paragraph Vector）**: CBOWに似た手法で、文書IDと周囲の単語から中央の単語を予測。文書全体の意味を保持したベクトルを得る。
  - **PV-DBOW（Distributed Bag of Words version of Paragraph Vector）**: Skip-gramに似た手法で、文書IDから文書中の単語を予測。文書IDのベクトルがその文書の意味を反映する。

## Attention (2014)
- **提案者**: Dzmitry Bahdanau ほか
- **内容**: Encoder-Decoderモデルで、入力の各部分に対して異なる重要度を持たせるメカニズム。これにより、長い入力でも重要な部分の情報を圧縮せずに保持できる。
  - **Encoder-Decoder**: 2つのニューラルネットワークで構成されるアーキテクチャ。Encoderが入力を処理し、Decoderが出力を生成する。翻訳などで使われる。

## GloVe (2014)
- **提案者**: Jeffrey Pennington ほか
- **内容**: 共起行列を用いた単語埋め込みモデル。各単語の出現頻度とその周囲の単語の情報を利用して、単語の意味をベクトル化する。
  - **共起行列**: 単語が他の単語と同時に出現する頻度を行列として表現したもの。単語間の意味的な関係を捉えるのに役立つ。

## Seq2Seq (2014)
- **提案者**: Ilya Sutskever ほか
- **内容**: 入力の系列データ（例：文）を出力系列に変換するモデル。LSTMを用いたEncoder-Decoder構造で、文章の翻訳やテキスト生成などに使用される。
  - **LSTM（Long Short-Term Memory）**: RNNの一種で、長期依存関係を保持しやすくするためのメカニズム。過去の重要な情報を忘れずに保持できる。

## fastText (2016)
- **提案者**: Piotr Bojanowski ほか
- **内容**: Word2Vecの改良版で、単語の一部（subword）に基づいてベクトルを学習することで、未知語にも強いモデル。
  - **Subword（部分語）**: 単語を小さな単位（例えば、接頭辞や接尾辞）に分解する手法。未知語にも対応できる。

## SCDV (2016)
- **提案者**: Dheeraj Mekala ほか
- **内容**: 単語ベクトルをGaussian Mixture Modelsでクラスタリングし、それを基に文書ベクトルを生成する。精度の高い文書表現を得られる。
  - **Gaussian Mixture Models（GMM）**: 正規分布を使ったクラスタリング手法。各クラスタの確率に基づいて、データポイントの所属確率を決定する。

## GNMT (2016)
- **提案者**: Yonghui Wu ほか
- **内容**: Googleのニューラル機械翻訳モデルで、seq2seqの大規模版。長文翻訳の精度を向上させるための技術が組み込まれている。

## Transformer (2017)
- **提案者**: Ashish Vaswani ほか
- **内容**: Attentionメカニズムのみで構成されたモデルで、RNNを使わずに並列処理が可能。BERTやGPTなどの基礎技術として広く使われている。
  - **Self-Attention（自己注意）**: 各単語が他の単語に対して持つ重要度を計算し、各単語の特徴を調整するメカニズム。並列処理に適している。

## ELMo (2018)
- **提案者**: Matthew E. Peters ほか
- **内容**: 文脈に基づいた単語ベクトルを生成するモデル。Bi-directional LSTMを用いて、前後の文脈を考慮し、単語の意味を変化させる。

## GPT (2018)
- **提案者**: Alec Radford ほか
- **内容**: 大量のテキストで事前学習された自己回帰型モデルで、各タスクへの適応が可能。転移学習を活用する。
  - **自己回帰型モデル**: 各時点での出力が、以前の出力に依存するモデル。次の単語を順に予測する。

## BERT (2018)
- **提案者**: Jacob Devlin ほか
- **内容**: 双方向Transformerを用いて、マスクされた単語を予測する事前学習タスクで学習する。様々なNLPタスクにおいてSOTA性能を記録。
  - **双方向Transformer**: 文章全体を同時に処理し、前後の文脈を同時に考慮するモデル。

## MT-DNN (2019)
- **提案者**: Xiaodong Liu ほか
- **内容**: マルチタスク学習を用いたモデルで、複数の自然言語処理タスクを同時に学習。BERTを基にしている。

## GPT-2 (2019)
- **提案者**: Alec Radford ほか
- **内容**: GPTの改良版で、大規模データで学習。自然な文章生成が可能で、悪用の懸念から当初は公開が制限されていた。

## ERNIE (2019)
- **提案者**: Yu Sun ほか
- **内容**: BERTを基に、中国語に特化したモデル。特定言語での知識を統合して性能向上を目指す。

## XLNet (2019)
- **提案者**: Zhilin Yang ほか
- **内容**: BERTの事前学習方法を改良し、双方向のオートレグレッシブモデリングを導入。BERTよりも高精度。

## RoBERTa (2019)
- **提案者**: Yinhan Liu ほか
- **内容**: BERTのパラメータチューニングを改良し、性能をさらに向上。複数のNLPタスクで優れた結果を出している。

## ALBERT (2019)
- **提案者**: Zhenzhong Lan ほか
- **内容**: BERTの軽量化バージョン。パラメータを削減しつつ高性能を維持している。

## T5 (2019)
- **提案者**: Colin Raffel ほか
- **内容**: Text-To-Text Transfer Transformer。すべてのNLPタスクをテキストの変換タスクとして学習し、多様なタスクに柔軟に対応可能。

## GPT-3 (2020)
- **提案者**: OpenAI
- **内容**: 1750億パラメータの巨大な言語モデルで、ほとんどのNLPタスクをゼロショットや少数ショットで実行可能。GPT-2から大幅に規模が拡大し、幅広い応用が可能に。
  - **Few-Shot Learning**: タスクに関する少量のサンプルだけで新しいタスクに対応できる学習方法。
  - **Zero-Shot Learning**: タスクに関するサンプルなしで、新しいタスクに対応できる学習方法。

## BART (2020)
- **提案者**: Facebook AI
- **内容**: BERTとGPTの要素を組み合わせたエンコーダー-デコーダーモデルで、要約や翻訳などのタスクに適用。破損したテキストの補完にも強い。

## Electra (2020)
- **提案者**: Google Research
- **内容**: 置換検出による事前学習手法で、少ない計算量でBERTに匹敵する性能を実現。BERTよりも効率的にトレーニングできる。

## DeBERTa (2021)
- **提案者**: Microsoft Research
- **内容**: トークンの位置情報と内容情報を分離する「ディスエンタンゴルド・アテンション」を活用し、文の意味理解を強化。様々なNLPタスクでSOTAを更新。

## GShardとSwitch Transformer (2021)
- **提案者**: Google Research
- **内容**: Mixture of Expertsアーキテクチャにより、数兆規模のパラメータを持つモデルを実現。各タスクに最適なエキスパートを動的に活性化し、計算資源を効率的に使用。

## DALL-E (2021)
- **提案者**: OpenAI
- **内容**: テキストから画像を生成するモデル。GPT-3のような言語モデルを視覚に応用し、テキストを元に独創的な画像を作成可能。

## CLIP (2021)
- **提案者**: OpenAI
- **内容**: テキストと画像の関連付けを学習し、テキストによる画像検索や分類が可能。視覚と言語をつなぐクロスモーダル表現を獲得。

## LaMDA (2021)
- **提案者**: Google Research
- **内容**: 自然な対話を目指すトランスフォーマーモデルで、文脈を保ちながら質問に回答することに特化している。

## GPT-4 (2023)
- **提案者**: OpenAI
- **内容**: GPT-3をさらに発展させ、マルチモーダル対応（テキストと画像）と高度な対話能力を備えたモデル。さまざまな対話と生成タスクに対応。

# 関係性図

```mermaid
graph TD
  subgraph 初期モデル
    A[自然言語処理の歴史] --> B[RNNLM]
    A --> C[Word2Vec]
    B --> D[Doc2Vec]
    C --> D
  end

  subgraph Attentionの発展
    D --> E[Attention]
    E --> H[Transformer]
    H --> L[GPT]
    H --> M[BERT]
  end

  subgraph Embeddingsと文書表現
    C --> F[GloVe]
    F --> I[fastText]
  end

  subgraph 系列モデルと翻訳
    B --> G[Seq2Seq]
    G --> J[GNMT]
  end

  subgraph Transformerと大規模モデル
    M --> R[RoBERTa]
    M --> S[ALBERT]
    M --> N[MT-DNN]
    M --> P[ERNIE]
    M --> Q[XLNet]
    M --> X[DeBERTa]
  end

  subgraph GPTの拡張と生成モデル
    L --> O[GPT-2]
    O --> U[GPT-3]
    U --> AC[GPT-4]
  end

  subgraph BERTの拡張
    M --> V[BART]
    V --> W[Electra]
  end

  subgraph LLMとマルチモーダル　　 
    H --> T[T5]
    T --> Y[Switch Transformer]
    H --> Z[DALL-E]
    H --> AA[CLIP]
  end

  subgraph 対話特化モデル
    X --> AB[LaMDA]
  end

  %% Define Styles for Better Visibility
  classDef attention fill:#FFDDC1,stroke:#333,stroke-width:2px,color:#000;
  classDef embeddings fill:#C1E1FF,stroke:#333,stroke-width:2px,color:#000;
  classDef languageModels fill:#FFECB3,stroke:#333,stroke-width:2px,color:#000;
  classDef transformers fill:#C8E6C9,stroke:#333,stroke-width:2px,color:#000;

  %% Apply Styles
  class B attention;
  class C embeddings;
  class D embeddings;
  class E attention;
  class F embeddings;
  class G languageModels;
  class H transformers;
  class I embeddings;
  class J languageModels;
  class K embeddings;
  class L transformers;
  class M transformers;
  class N transformers;
  class O transformers;
  class P transformers;
  class Q transformers;
  class R transformers;
  class S transformers;
  class T transformers;
  class U transformers;
  class V transformers;
  class W transformers;
  class X transformers;
  class Y transformers;
  class Z transformers;
  class AA transformers;
  class AB transformers;
  class AC transformers;
